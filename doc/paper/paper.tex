\documentclass{article}

\usepackage{epsfig}
\usepackage[margin=1in]{geometry}
\usepackage[round]{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[capitalise]{cleveref}
\usepackage{verbatim}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{color}

\newcommand{\bO}{\mathcal{O}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\ostu}{\operatorname{stu}}
\newcommand{\oref}{\operatorname{ref}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg\;min}}\;}
\newcommand{\todo}[1]{{\color{red} (TODO: #1)}}

\title{
  Fast and robust ancestry inference
  using principal component analysis
}
\author{Daiwei Zhang, Rounak Dey, Sehee Kim, Seunggeun Lee}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Population stratification (PS) is a major confounder in genome-wide association studies (GWAS) and can lead to false positive associations. To adjust for PS, principal component analysis (PCA)-based ancestry prediction has been widely used. This approach first performs PCA on the reference samples with detailed ancestry information and then uses the reference sample PCA results to predict the PC scores of the study samples. Simple Projection (SP) and Augmentation-Decomposition-Transformation (ADP), such as LASER and TRACE, are popular methods for predicting PC scores. However, they are either biased or computationally expensive. For example, the predicted PC scores from SP can be biased toward NULL. On the other hand, since ADP requires running PCA separately for each study sample on the augmented data set, its computational cost is high.

To address these problems, we propose two alternative approaches, Adjusted Projection (AP) and Online ADP (OADP), implemented in the genoPCA software. Using random matrix theory results, AP asymptotically estimates and adjusts for the bias of SP. Moreover, OADP uses online singular value decomposition, which can greatly reduce the computation cost of ADP. We carried out extensive simulation studies to show that these alternative approaches are unbiased and that the computation times are 10-100 times faster than ADP when 2000 reference samples are used. We also applied our approaches to 500,000 study samples from the UK Biobank data with 2,500 samples from the 1000 Genomes data as the reference. AP and OADP required 16 and 90 CPU hours, respectively, while the projected computation time of ADP is 1,828 CPU hours. Furthermore, when we only used the European reference samples in the 1000 Genomes to infer sub-European ancestry, SP clearly showed bias, unlike the proposed approaches. By using AP and OADP, we can infer ancestry and adjust for PS in GWAS robustly and efficiently   
    
\end{abstract}

\section{Introduction}

Population stratification (PS) is an important confounder for the association between
genotypes and phenotypes \citep{price2006principal}.
To adjust for PS, we need to accurately estimate the ancestry structure in the study samples.
Principle component analysis (PCA) is a multivariate statistical method
which finds the direction of the maximum variability among the samples.
By aggregating information across all genetic markers,
PCA can accurately infer the ancestry structure \citep{reich2008principal}.
To adjust for PS,
PCA can be applied to study data to calculate the principle component (PC) scores,
which are regarded as variables of ancestry direction
and can be used as covariates to adjust for.
An alternative approach for doing PCA is
predicting the PC scores of the study samples
by using some reference genotyped samples with detailed ancestry information.
This prediction-based approach allows not only adjustment for PS
but also inference of the ancestry memberships of the study samples. 

The standard way of predicting PC scores is to find the PC loadings of the reference samples and project the study samples onto them.
We call this method simple projection (SP).
However, when the number of features greatly exceeds the size of the reference samples,
which is common for genotype data,
the PC scores predicted by SP are known to be biased toward null \citep{dey2016asymptotic}.
One way of addressing this shrinkage bias is presented by \citet{wang2014ancestry, wang2015improved}.
Their solution is to combine one study sample with all the reference samples and
find the PC scores of this augmented data set by using singular value decomposition (SVD).
The PC scores of the study individuals are then retrieved from the singular vector matrices and mapped to the reference PC space by a Procrustes transformation.
We call this method ``augmentation, decomposition, and Procrustes transformation'' (ADP).
This method has been shown to be effective in eliminating the shrinkage bias of study PC scores.
However since ADP needs to run SVD for each study sample,
ADP is computationally expensive.

To address the limitations of SP and ADP,
we propose and evaluate two alternative PC score prediction methods.
The first approach removes the bias in SP by estimating the asymptotic bias factor,
which is calculated based on random matrix theory \citep{dey2016asymptotic}.
The second approach improves the computational efficiency of ADP
by using an online SVD algorithm \citep{halko2011finding}, 
which obtains the SVD results of the augmented matrix
by updating the SVD results of the reference matrix,
since the latter only differs slightly from the former
and the overlapping calculations can be avoided
(at least approximately).
We call the first approach ``adjusted projection'' (AP)
and the second approach ``online augmentation, decomposition, and Procrustes transformation'' (OADP). 
We found that AP and OADP have both achieved accuracy close to that of ADP
and computational efficiency close to that of SP.

\section{Method}

\subsection{Model}

\subsubsection{Data Format}
For PC score prediction,
we have a reference group and a study group.
The reference and study data sets are represented by matrices.
Let $X$ be a $p \times n$ matrix of reference genotypes
and  let $Y$ be a $p \times m$ matrix of study sample genotypes,
where $p$ is the number of genetic markers,
$n$ is the number of reference samples
and $m$ is the number of study samples.
In our study we only consider genotypes composed of biallelic single nucleotide polymorphisms (SNP),
so each entry of $X$ and $Y$ is a minor allele count of 0, 1, or 2.

\subsubsection{Principle Component Analysis on the Reference Data}
First, we conduct a PCA on the reference data $X$.
The reference data matrix is standardized
so that the mean and standard deviation of each feature are $0$ and $1$, respectively.
With $n$ data points residing in a $p$-dimensional vector space and $n < p$,
there is a unique $n$-dimensional hyperplane that covers all the data points.
This hyperplane can be represented as the span of an orthonormal basis composed of $n$ unit vectors.
Of course, the choice for such orthonormal basis is not unique,
but what what we are interested in PCA is 
to maximize the variance of the samples on the direction of the first basis element,
maximize the variance on the samples on the second basis element after they are collapsed in the direction of the first basis element,
and so forth,
until we have $n$ basis elements.
At the end, PCA gives us three variables:
\begin{enumerate}
    \item $n$ pairwise-orthogonal unit vectors of $p$ dimensions
    representing the orthonormal basis.
    These are called principle components (PC) or PC loadings,
    and we organized them into a $p \times n$ matrix $U$.
    \item $n$ non-negative real numbers
    each of which represents the standard deviations of the samples
    on the direction of the corresponding PC loading.
    They are organized into an $n \times n$ diagonal matrix $D$.
    \item $n$ tuples of $n$ dimensions
    representing the normalized coordinates of the data points
    in the $n$-dimensional space spanned by the PC loadings.
    These are called the normalized PC scores,
    and they are organized into an $n \times n$ matrix $V$,
    where each column corresponds to the coordinate of a PC loading
    and each row corresponds to a sample.
\end{enumerate}
If we multiply each normalized PC score by the corresponding standard deviation
(i.e. calculate $VD$),
we get the unnormalized PC scores.
In our study, we use unnormalized PC scores and simply call them PC scores.
The study can be carried out by using normalized PC scores, too,
and the result would be equivalent.
Moreover, the standard deviation matrix $D$ is in descending order,
so that the first column of $U$ is the direction of the greatest sample variance,
the first column of $V$ represents the coordinates of the $n$ samples in this direction.
The same correspondence applies to $U$'s and $V$'s second column, third column, etc.
Thus PCA reduces the dimension of the sample space from $p$ to $n$.
We can further reduces the sample space dimension
by using only the first $k$ PCs ($k \ll n$) and ignore the rest $n-k$ PCs.
In genetic studies, choosing $k \leq 10$ is often sufficient.

The (standardized) reference data matrix can be restored by $X = U D V^T$.
In fact, this is exactly the SVD of $X$ \citep{jolliffe2002principal}.
Thus PCA on the reference data can be done easily on any language
in which a SVD algorithm has been implemented.
If all that we are interested in is the PC scores,
we can reduce the computational complexity by applying eigen decomposition (EGD) on $X^TX$
to obtain $X^T X = V D^2 V^T$.
Afterwards, $U$ can be retrieved by $U = X V D^{-1}$ if needed.

\subsubsection{Predicting the PC Scores of the Study Samples}

Once PCA is finished on the reference data,
we predict the PC scores of the study data.
There are more than one ways to accomplish this,
and two common methods are SP and ADP.
The performance of these two methods can be improved by using AP and OADP.
We now discuss the algorithm, computational efficiency, and accuracy of these four methods.

Before we using any of these methods,
we need to standardize the study samples
by subtracting the reference means from them
and then dividing them by the reference standard deviations.
For simplicity,
we overload the symbol $Y$
and let it represent the standardized study data for the rest of the paper.

\paragraph{Simple Projection (SP)}
This method is simple and intuitive.
Since the PCA on the reference data has already given us the PC loadings,
we can simply project a (standardized) study sample onto them by calculating their dot products.
The resulted projection lengths are the predicted PC scores for this study sample.
The algorithm is

\begin{enumerate}
\item Compute the reference covariance matrix $X^T X$.
  (Computational complexity (CC): $\bO[pn^2]$.)  
\item EGD on the reference covariance matrix: $X^T X = V D^2 V^T$.
  (CC: $\bO[n^3]$.)
\item Compute the PC loading matrix for the top $k$ PCs only: $U_k= X V_k D_k^{-1}$.
Here $V_k$ and $D_k$
are the the first $k$ columns of $V$
and the upper-left $k \times k$ submatrix of $D$, respectively.
(CC: $\bO[pnk]$.)
\item Compute the predicted study PC scores for the top $k$ PCs: $W_k = Y^T U_k$.
(CC: $\bO[mpk]$.)
\end{enumerate}
The total computational complexity is  $\bO[pn^2 + mpk]$
given that $k \ll n \ll p$.
The computational complexity for each study sample is $\bO[pk]$,
which is the lowest among all the methods discussed in this paper.
However, though computationally efficient,
a major disadvantage of SP is the loss of accuracy when the number of features $p$ greatly exceeds the reference sample size $n$,
a situation that is not uncommon for genotype data.
\citet{dey2016asymptotic} have shown that when $n \ll p$,
especially when the signals in the reference samples are not strong,
the predicted PC scores will shrink toward the origin.
This shrinkage bias limits the accuracy of SP for high dimensional data.
Fortunately, \citet{dey2016asymptotic} have also provided a way
to estimate the asymptotic shrinkage magnitude and angle
and implemented it in the \verb|hdpca| package in the R language \citep{hdpca}.
We use this estimation to eliminate the shrinkage bias,
as in the AP method.

\paragraph{Adjusted Projection (AP)}

This method is almost identical to SP,
with the only difference being the addition of the adjustment for the shrinkage bias
after the projections are finished to restore the unshrinked PC scores.
For estimating the asymptotic shrinkage factor,
we need all the eigenvalues
of the the reference data matrix,
as well as the number of features
and the reference sample size.
The details for estimating the shrinkage factor can be found in \citet{dey2016asymptotic}.
The algorithm is summarized below.

\begin{enumerate}
\item Compute the reference covariance matrix $X^T X$.
  (CC: $\bO[pn^2]$.)  
\item EGD on the reference covariance matrix: $X^T X = V D^2 V^T$.
  (CC: $\bO[n^3]$.)
\item Estimate the shrinkage magnitudes and organize them in a $n \times n$ diagonal matrix $S$.
  (CC: $\bO[p]$.)
\item Compute the PC loading matrix for the top $k$ PCs with the adjustment for the shrinkage bias: $U_k= X V_k D_k^{-1} S_k^{-1}$.
Here $V_k$, $D_k$, and $S_k$
are the the first $k$ columns of $V$
and the upper-left $k \times k$ submatrices $D$ and $S$, respectively.
(CC: $\bO[pnk]$.)
\item Compute the predicted study PC scores for the top $k$ PCs: $W_k = Y^T U_k$.
(CC: $\bO[mpk]$.)
\end{enumerate}
Given that $k \ll n \ll p$,
the total computational complexity is $\bO[pn^2 + mpk]$,
which is the same as that of SP.
Thus the adjustment for the shrinkage bias in AP
does not increase the computational complexity of SP
(though it does slightly increase the empirical runtimes).
A minor disadvantage of AP is that
while the computational complexity and memory usage of SP
can be further reduced
by using some truncated SVD algorithm
(such as the randomized SVD algorithm
by \citet{halko2011finding}
to compute the SVD for only the top $k$ PCs of the reference matrix,
AP requires all the eigenvalues
and thus a full SVD or EGD of the reference matrix.

\paragraph{Augmentation, Decomposition, and Procrustes Transformation (ADP)}

ADP predicts the study PC scores by using a completely different approach
compared to SP and AP.
The details of this method
and an implementation in C++ called TRACE
are described in \cite{wang2014ancestry,wang2015improved}.
In ADP,
we augment the (standardized) reference matrix by one column
by appending a (standardized) study sample to it.
Then we apply SVD or EGD to this $p \times (n+1)$ matrix $\tilde{X}$.
The resulted $(n+1) \times (n+1)$ normalized PC score matrix $\tilde{V}$
can be divided into two submatrices:
the first $n$ rows $\tilde{V}_{\oref}$,
which correspond to the reference samples,
and the last row $\tilde{v}_{\ostu}$,
which corresponds to the study sample.
Notice that $\tilde{V}_{\oref}$ is now different from $V$,
the $n \times n$ normalized PC score matrix of the reference data,
so we look for a linear transformation
\[
    f(\tilde{V}_{\oref}) = \rho A \tilde{V}_{\oref} + c
\]
where $\rho$ is a non-negative real number
(for uniform scaling),
$A$ is a $n \times (n+1)$ orthonormal matrix
(for projection, rotation and reflection),
and $c$ is a column vector
(for translation),
so that the distance
between $V$ and the transformed $f(\tilde{V}_{\oref})$
(i.e. the sum of squares of the error matrix $V - f(\tilde{V}_{\oref})$)
is minimized.
This is called the Procrustes transformation
from $f(\tilde{V}_{\oref})$ to $V$.
We then apply this transformation to $\tilde{v}_{\ostu}$
to obtain $f(\tilde{v}_{\ostu})$.
Finally,
by multiplying these Procrustes-transformed normalized PC scores
by the reference PC standard deviations,
we obtain the predicted PC score for the study sample $f(\tilde{v}_{\ostu})D$.

Notice that so far we have been using all the PCs for ADP.
However, this is not necessary
if all that we are interested in
is the top few PCs.
Thus we can use only
the top $k$ PC scores of the reference matrix
and the top $lk$ PC scores of the augmented matrix
(where $l$ is a constant greater than or equal to $1$ and $lk \ll n$)
and find the Procrustes transformation from $\bR^{lk}$ to $\bR^k$.
The algorithm is summarized as follows.

\begin{enumerate}
\item Compute the reference covariance matrix $X^T X$.
  (CC: $\bO[pn^2]$.)  
\item Apply EGD to the reference corariance matrix
  to obtain $X^T X = V D^2 V^T$.
  (CC: $\bO[n^3]$.)
\item For a study sample $y$,
  find the augmented covariance matrix
  $\tilde{X}^T \tilde{X}$
  by computing $X^T y$, $(X^T y)^T$, and $y^T y$
  and appending them to the
  bottom edge,
  right edge,
  and bottom-right corner of $X^TX$.
  (CC: $\bO[pn]$.)
\item Apply EGD to $\tilde{X}^T \tilde{X}$ to get $\tilde{X}^T \tilde{X} = \tilde{V} \tilde{D}^2 \tilde{V}^T$.
  (CC: $\bO[n^3]$.)
\item Find the Procrustes transformation
from the first $lk$ columns of $\tilde{V}$
to the first $k$ columns of $V$.
  (CC: $\bO[nk^2]$)
  \item Go to step 3 for the next study sample
  until all the study samples are analyzed.
\end{enumerate}
The total computational complexity is
$\bO[pn^2 + m(np + n^3)]$
given that $lk \ll n \ll p$.

ADP is regarded as the most accurate method for predicting study PC scores \todo{Find citation}.
It does not have SP's shrinkage bias.
However, the major disadvantage of ADP
is its high computational complexity.
In particular,
as the reference size increases,
the computational cost for a study sample
increases cubicly.
Fortunately, this disadvantage can be mitigated by taking some computational shortcuts,
as in the OADP method.

\paragraph{Online Augmentation, Decomposition, and Procrustes Transformation (OADP)}

Notice that the augmented data matrix $\tilde{X}$
only differs slightly from the reference matrix $X$
when the reference sample size is large.
Thus the computational process for the SVD of $\tilde{X}$
is numerically close to that for the SVD of $X$.
If we can avoid the repeated computation in this process
and obtain the SVD of $\tilde{X}$
by updating the SVD of $X$
instead of recalculating everything,
we could greatly reduce the computational cost.
One of such ``online'' algorithms for computing the SVD of the augmented matrix is discussed in \citet{brand2002incremental}.
This algorithm calculates SVD in the incremental manner described above
and has the ability to find the approximate SVD for the top few singular values only,
which exactly fits our need for obtaining the top few PCs.
Thus we use the algorithm by \citet{brand2002incremental}
to replace the standard algorithm for decomposing the augmented matrices
and coin the name ``online augmentation, decomposition, and Procrustes transformation'' (OADP).
The algorithm for this method is as follows:

\begin{enumerate}
\item Compute $X^T X$.
  (Computational complexity: $\bO[n^2p]$.)  
\item Apply EGD to $X^T X$ to get $X^T X = V D^2 V^T$,
  where $D$ and $V$ only need their first $k$ columns.
  ($\bO[n^3]$.)
\item Calculate $U = X V D^{-1}$ but for the first $k$ columns only.
  ($\bO[k n p]$.)
\item Calculate 
  \[
    L = U^T y \quad \text{and} \quad K = y^T H,
  \]
  where $H$ is the normalized  $y - UL$
  ($\bO[k p]$.)
\item Calculate $Q^T Q$, where
  \[
    Q = 
    \begin{bmatrix}
      D & L \\
      0 & K
    \end{bmatrix}.
  \]
  ($\bO[k^3]$.)
\item Apply EGD to $Q^T Q$ to get $Q^T Q = \ddot{V} \ddot{D}^2 \ddot{V}^T$.
  ($\bO[k^3]$.)
\item Calculate
  \[
    \tilde{V} =
    \begin{bmatrix}
      V & 0 \\
      0 & 1
    \end{bmatrix}
    \ddot{V}.
  \]
  ($\bO[nk^2]$.)
\item Transform the first $k_2$ columns of $\tilde{V}$ to the first $k_1$ columns of $V$ by using Procrustes analysis to get the transformed PC scores of the study individual,
  where $k_1 < k_2  < k \ll n$.
  (Computational complexity is negligible if $k_2$ is small compared to $n$.)
  \item Go to step 4 for the next study individual.
\end{enumerate}

The total computational complexity is $\bO[n^2p + m(kp + k^2n)]$
provided $k \ll n \ll p$.
The computational complexity of OADP for the study individuals increases linearly with respect to the reference sample size,
which is much more efficient than ADP's cubic increasing rate.
The closeness between the results given by OADP and ADP
will be empirically shown in \Cref{sec:results}.

A comparison of computational complexities of the four PCA methods are shown in \Cref{tbl:cplx}.

\subsection{Simulation Studies}

We used the Grid Genotype Simulator (GGS)
by \citet{mathieson2012differential}
to simulate population migration.
The grid's dimension is 2 by 2
with the same number of individuals
in each of the 4 populations.
Moreover, we let the number of reference samples
range from 1000 to 3000
and fix the number of study samples to 200.
Each genotype sample has 100 genealogies
with 1000 loci in each genealogy.
In addition, the migration rate was set to 100.
After the individuals were simulated,
we applied SP, ADP, AP, and OADP to the data
to predict the PC scores for the study samples.
We only calculated the top 4 PCs,
and for OADP and ADP,
we calculated the top 20 PC scores for the study samples
and projected them to the dim-4 reference PC score space
with a Procrustes transformation.
Finally, we used the k-nearest neighbors method
to predict which ethnic group a study sample comes from.

We used two measures for the accuracy of PC score prediction.
First, we measured the ADP error,
which is the Euclidean distance
from a method's predicted PC scores
to ADP's PC scores,
since ADP is believed to be the currently most reliable method.
Next, for the center error,
we calculated the centers of mass of each population's PC scores.
This was done for both the reference and the study samples.
Then the center error
was set to be the Euclidean distance
from the reference centers to the corresponding study centers.

For the comparison of computational cost,
we recorded each method's runtime for analyzing the study samples.
(The runtime for PCA on the reference samples
and for reading or writing files are not included.)
We used our genoPCA software,
which implemented SP, AP, and OADP in Python.
and TRACE by \citet{wang2015improved},
which implemented ADP in C++.
All the programs were run on a single CPU core.

\subsection{UK Biobank data analysis}

We also applied the four methods
to study the UK Biobank data \citep{sudlow2015uk},
which consists of 488,000 middle- or elderly-aged individuals
in the United Kingdom.
Our reference samples
are from the 1000 Genomes Project \citep{10002015global},
which collected 2500 samples from Europe, Africa, East Asia, South Asia, and America.
(Individuals whose children are also in the data were removed.)
The reference and the study samples
have 145,000 SNPs in common.
After we predicted the PC scores and ethnic groups
of the study samples,
we selected the study samples predicted to by Europeans
and applied PCA to them
by using the European reference samples only.
Since ADP is very slow compared to the other three methods,
we randomly selected 500 study samples
and used them to compare the four methods.
After that,
we applied SP, AP, and OADP to all the study samples.
The same measurements for accuracy and computational cost
from the simulation study
was applied here.

\section{Results}\label{sec:results}.

\subsection{Simulation studies}

For the simulation study,
the PC scores predicted by AP, OADP, and ADP
are very close to each other
for every choice of reference sample size (\Cref{fig:ggsim}).
Compared to their results,
SP exhibits a strong shrinkage
when the reference sample size is 1000.
As we gradually increases the reference sample size to 3000
by an increment of 500,
the magnitude of the shrinkage decreases
but is still visible.
This is consistent with the theoretical result
that the shrinkage bias decreases
as the ratio of
the reference sample dimension to the reference sample size
decreases \citep{dey2016asymptotic}.

The accuracy of the four methods were compared
by using their ADP errors and center errors (\Cref{tbl:ggsim}).
Compared to SP,
the other three methods' errors are very small
and close to each other.
OADP had smaller errors than AP
for both the ADP errors and the center errors.
Interestingly, the center errors of ADP were slightly higher
than that of OADP.
As the reference size increased,
the errors of all the four methods decreased.
SP exhibited the fastest decrease in both errors,
and the four methods' difference in accuracy
became smaller and smaller (\Cref{fig:ggsim_trend}).

For computational cost (\Cref{tbl:ggsim}),
we observed that the runtime for ADP increases faster than linearly
as the reference sample size increases.
This is consistent
with the theoretical computational complexity of ADP,
which expects the runtime to increase at the rate of $\bO[n^3]$ (\Cref{tbl:cplx}).
When the reference sample size reached 3000,
ADP's runtime was 27 times of OADP's
and 225 times of SP's and AP's.
In comparison, the runtimes of SP, AP, and OADP
increased only slightly,
which is consistent with their $\bO[1]$ and $\bO[n]$ computational complexity when the data dimension and study sample size are fixed.
SP and AP were the most computational efficient methods.
Their runtimes were approximately only one-eighth of OADP's.

\subsection{UK Biobank data}

When we used the 500 randomly selected global samples
in the UK Biobank data,
we found that the PC scores predicted by the four methods' are almost identical (\Cref{fig:ukb_500}).
The separation of the ethnic groups was clear,
and the shrinkage bias of SP was not obvious,
which is consistent with the theoretical understanding
that the shrinkage bias decreases
as the signal-to-noise ratio increases \todo{Find citation}.

However, when we focused on the European (EUR) samples
among these 500 UK Biobank samples,
(of which there are 475 many,)
the results given by SP were clearly inaccurate (\Cref{fig:ukb_500_eur}).
The samples from British in England and Scotland (GBR)
and those from Utah residents with northern and western European ancestry	(CEU)
drifted very close to NULL.
This caused SP to incorrectly classified almost all of them as CEU,
since most GBR reference samples are farther away from NULL.
A few of the study samples predicted by ADP to be GBR or CEU
were even classified as the Iberian population in Spain (IBS),
since the IBS reference samples are close to NULL.
Moreover, the shrinkage for the IBS
and for the Toscani in Italia (TSI) populations
caused significant errors within these two groups.
The IBS reference samples
are approximately in the middle
between the TSI reference samples and NULL.
Thus most of the the study samples predicted by ADP to be TSI
were classified by SP as IBS.

For AP and OADP, their predicted PC1 and PC2 scores
for the European study samples
were almost identical
to those done by ADP.
The three methods' classification
agreed on most of the samples,
except for within the cluster of CEU and GBR.
Since many residents with northern and western European ancestry living in Utah
could originally migrated from Britain,
these two groups' ancestral history could be very similar
and thus difficult if not impossible to be separated by PCA.
Hence any slightest change
in the PC scores for these two groups' study samples
could easily change the classification from one group to the other.
This is a problem inherent to the population structure
and is not caused by the limitations of our methods.
The patterns discussed above
were still observed
when we used all the 488,000 study samples in the UK Biobank data set
and those predicted to be Europeans among them (\Cref{fig:ukb,fig:ukb_eur}).


Just as in the simulation study,
SP's errors were the greatest (\Cref{tbl:ukb}).
Its ADP error
was twice of SP's and 9 times of OADP's for the global samples,
and 14 times of SP's and OADP's for the European samples.
For the center errors,
SP's error was still much greater than the other three methods.
For the European samples,
ADP's center error was the lowest,
followed by OADP and AP.
Interestingly, this order of the three methods
was reversed for the global samples.

For the computational cost,
ADP's runtime was 1.5 times of OADP's and 10 times of AP's and SP's
when only the European samples
of the 500 random global study samples
and the 498 European reference samples of the 1000 Genomes data
were used (\Cref{tbl:ukb}).
This difference increased to 18 times, 103 times, and 108 times
when we studied the all the 500 random global samples
with all the 2492 1000 Genomes reference samples.
As we included all the 488,000 study samples in the UK Biobank data,
SP, AP, and OADP only took 13, 16, and 90 CPU hours,
while ADP was estimated (from its 500-sample runtime)
to take 1828 CPU hours,
which makes ADP very computational costly to use in practice.

\section{Discussion}

From the simulation study and the study on the UK Biobank data,
we see that AP and OADP have achieved the accuracy of ADP
and the computational efficiency of SP.
The advantage of AP's and OADP's accuracy
is the most clear
when the ratio of the reference sample size
to the number of features is low
or the signal-to-noise ratio is low.
On the other hand,
the advantage of AP's and OADP's computational efficiency
is the most observable when the reference sample size is large.
After the comparison of the four methods,
we conclude that for a very small reference size,
SP is unreliable since the shrinkage would be great,
and ADP is a decent method to use
because its computational inefficiency is negligible,
but AP and OADP would still be much faster than ADP
and their accuracy would be very close to ADP's.
As the reference sample size increases
(with respect to the number of variants),
all the four methods' accuracy converge together,
and SP becomes more and more accurate,
although for genotype data,
due to its high dimensions,
the reference size must be extremely large
in order for SP to achieve an accuracy
that is at the same level as the other three methods.
Therefore, for the sample sizes
of the data sets that we have currently,
OADP and SP are the most robust and fast methods
for ancestry inference by PCA.

For the future, we would like
to improve the parallelism of our genoPCA software.
We are also interested in applying randomized SVD algorithms
such as the one by \citet{halko2011finding}
for PCA on the reference data,
since as the reference sample size becomes larger and larger,
it becomes impractical to load such a large matrix into memory
to compute its SVD or EGD.

\newpage

\appendix
\bibliographystyle{plainnat}
\bibliography{paper}

\newpage
\section{Tables and Figures}

\begin{figure}[p]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1000_100000_200_2_1_100_1_sturef_ggsim1000_100000_1000_2_1_100_0_sp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1000_100000_200_2_1_100_1_sturef_ggsim1000_100000_1000_2_1_100_0_ap}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1000_100000_200_2_1_100_1_sturef_ggsim1000_100000_1000_2_1_100_0_oadp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1000_100000_200_2_1_100_1_sturef_ggsim1000_100000_1000_2_1_100_0_adp}
\end{subfigure}
\end{figure}

\begin{figure}[p]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1500_100000_200_2_1_100_1_sturef_ggsim1500_100000_1500_2_1_100_0_sp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1500_100000_200_2_1_100_1_sturef_ggsim1500_100000_1500_2_1_100_0_ap}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1500_100000_200_2_1_100_1_sturef_ggsim1500_100000_1500_2_1_100_0_oadp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim1500_100000_200_2_1_100_1_sturef_ggsim1500_100000_1500_2_1_100_0_adp}
\end{subfigure}
\end{figure}

\begin{figure}[p]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2000_100000_200_2_1_100_1_sturef_ggsim2000_100000_2000_2_1_100_0_sp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2000_100000_200_2_1_100_1_sturef_ggsim2000_100000_2000_2_1_100_0_ap}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2000_100000_200_2_1_100_1_sturef_ggsim2000_100000_2000_2_1_100_0_oadp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2000_100000_200_2_1_100_1_sturef_ggsim2000_100000_2000_2_1_100_0_adp}
\end{subfigure}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2500_100000_200_2_1_100_1_sturef_ggsim2500_100000_2500_2_1_100_0_sp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2500_100000_200_2_1_100_1_sturef_ggsim2500_100000_2500_2_1_100_0_ap}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2500_100000_200_2_1_100_1_sturef_ggsim2500_100000_2500_2_1_100_0_oadp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim2500_100000_200_2_1_100_1_sturef_ggsim2500_100000_2500_2_1_100_0_adp}
\end{subfigure}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim3000_100000_200_2_1_100_1_sturef_ggsim3000_100000_3000_2_1_100_0_sp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim3000_100000_200_2_1_100_1_sturef_ggsim3000_100000_3000_2_1_100_0_sp}
\end{subfigure}
\hfill
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim3000_100000_200_2_1_100_1_sturef_ggsim3000_100000_3000_2_1_100_0_oadp}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.98\linewidth]{img/ggsim3000_100000_200_2_1_100_1_sturef_ggsim3000_100000_3000_2_1_100_0_adp}
\end{subfigure}
\caption{
  The PC scores predicted by each of the four methods
  as the reference size varies.
  Reference samples are in colors
  and study samples are in black.
  The stars mark the centers of mass (geographic centers) of each population for the reference and study samples.
  The number of variants is 100,000,
  and the study sample size is 200.
  Only the top 4 PCs are calculated.
  The data is simulated by the GGS software \citep{mathieson2012differential}.
  \label{fig:ggsim}
}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Method & Reference Complexity & Study Complexity \\ 
    \hline
    ADP & $\bO[n^2 p]$ & $\bO[mn(p + n^2)]$ \\
    \hline
    OADP & $\bO[n^2 p]$ & $\bO[mk(p + k n)]$ \\
    \hline
    SP & $\bO[n^2p]$ & $\bO[mkp]$ \\
    \hline
    AP & $\bO[n^2p]$ &  $\bO[mkp]$ \\
    \hline
  \end{tabular}
  \caption{
    Comparison of computational complexity for different methods.
    Here $p$ is the number of SNPs,
    $n$ is the number of individuals in the reference group,
    $m$ is the number of individuals in the  study group,
    and $k$ is the number of PCs calculated by the corresponding method.
  }
  \label{tbl:cplx}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Reference Size & 1000 & 1500 & 2000 & 2500 & 3000 \\
    \hline
    \multicolumn{6}{|c|}{Runtime (sec)} \\
    \hline
    SP         &  17  & 17   & 16   & 18   & 18   \\
    AP         &  18  & 16   & 18  & 16   & 17  \\
    OADP       &  145  & 146   & 147   & 148   & 149   \\
    ADP        &  379  & 809 & 1526 & 2556 & 4062 \\
    \hline
    \multicolumn{6}{|c|}{Error (ADP)} \\
    \hline
    SP         &  188  & 139   & 112   & 91   & 84   \\
    AP         &  31  & 27   & 12  & 11   & 14  \\
    OADP       &  21  & 16   & 12   & 9   & 10   \\
    ADP        &  0  & 0 & 0 & 0 & 0 \\
    \hline
    \multicolumn{6}{|c|}{Error (centers)} \\
    \hline
    SP         &  335 & 161  & 113 & 71  & 60  \\
    AP         &  31  & 19   & 17  & 25  & 15  \\
    OADP       &  12  & 17   & 11  & 20  & 12  \\
    ADP        &  20  & 14   & 14  & 23  & 14  \\
    \hline
  \end{tabular}
  \caption{
    The study runtimes and errors of the four methods
    as the reference size increases.
    Runtimes are measured in seconds.
    Errors (ADP) are the Euclidean distances from
    the PC scores predicted by each method
    to those predicted by ADP.
    Errors (centers) are the Euclidean distances
    from the centers of mass of the reference samples
    to the centers of mass of the study samples.
    The number of variants is 100,000,
    and the study sample size is 200.
    Only the top 4 PCs are calculated.
    The data is simulated by the GGS software \citep{mathieson2012differential}.
    }
    \label{tbl:ggsim}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=.90\linewidth]{img/ggsim_runtimes.png}
  \centering
  \includegraphics[width=.90\linewidth]{img/ggsim_errors_adp.png}
  \centering
  \includegraphics[width=.90\linewidth]{img/ggsim_errors_ctr.png}
  \caption{
    The runtimes and errors of the four methods
    as the reference size increases.
    Runtimes are measured in seconds.
    Errors from ADP are the Euclidean distance
    from each method's result to ADP's result.
    Errors between centers are the Euclidean distance
    from the centers of mass of the reference populations
    to the corresponding centers of mass of the study populations.
    The number of variants is 100,000,
    and the study sample size is 200.
    Only the top 4 PCs are calculated.
    The data is simulated by the GGS software \citep{mathieson2012differential}.
    \label{fig:ggsim_trend}
  }
\end{figure}

\begin{table}[H]
  \centering
  \begin{tabular}{|r|r|r|r|r|r|r|r|}
    \hline
    & \multicolumn{3}{|c|}{Runtime} & \multicolumn{2}{|c|}{Error (ADP)} & \multicolumn{2}{|c|}{Error (centers)} \\
    \hline
    Samples & All (hrs) & Global (sec) & European (sec) & Global & European & Global & European \\
    \hline
    Sample size & 488,000 & 500 & 475 & 500 & 475 & 500 & 475 \\
    \hline
    SP   & 13    &  62   & 48  & 92  & 450 & 3156 & 2304 \\ 
    AP   & 16    &  65   & 48  & 48  & 33  & 2486 & 706  \\
    OADP & 90    &  363  & 349 & 10  & 32  & 2575 & 665  \\
    ADP  & 1828*  &  6739 & 518 & 0   & 0   & 2578 & 649  \\
    \hline
  \end{tabular}
  \caption{
    The study runtimes and erros for the four methods
    for 1) all the samples and 2) 500 randomly selected study samples
    in the UK Biobank data.
    Errors (ADP) are the Euclidean distances from
    the PC scores predicted by each method
    to those predicted by ADP.
    Errors (centers) are the Euclidean distances
    from the centers of mass of the reference populations
    to the corresponding centers of mass of the study populations
    predicted by the $k$-nearest neighbor method.
    For the global study,
    2492 samples from the 1000 Genomes data are used as the reference.
    For the European study,
    only the 498 European samples among them are used.
    The number of variants
    in common between the reference and study samples is 145,282.
    Only the top 4 PCs are calculated.
    *Estimated from the runtime of ADP for the 500 global samples.
    }
    \label{tbl:ukb}
\end{table}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.90\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_sp}
  \includegraphics[width=0.90\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_ap}
  \includegraphics[width=0.90\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_oadp}
  \includegraphics[width=0.90\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_adp}
  \caption{PC scores of 500 randomly selected global samples in UK Biobank data (circles) predicted by the four methods with the global samples in the 1000 Genomes data (squares) as the reference.}
  \label{fig:ukb_500}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.9\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_pred_EUR_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_sp}
  \includegraphics[width=0.9\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_pred_EUR_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_ap}
  \includegraphics[width=0.9\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_pred_EUR_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_oadp}
  \includegraphics[width=0.9\textwidth]{img/ukb_snpscap_kgn_bial_orphans_5c_pred_EUR_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_adp}
  \caption{PC scores of the European in the 500 randomly selected global samples UK Biobank data (circles) predicted by the four methods with the European samples in the 1000 Genomes data (squares) as the reference.}
  \label{fig:ukb_500_eur}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_sp.png}
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_ap.png}
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_oadp.png}
  \caption{All the 488366 samples in the UK Biobank data.}
  \label{fig:ukb}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_pred_EUR_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_sp.png}
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_pred_EUR_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_ap.png}
  \includegraphics[width=0.98\textwidth]{img/ukb_snpscap_kgn_bial_orphans_pred_EUR_nchunks100_sturef_kgn_bial_orphans_snps_ukb_snpscap_ukb_EUR_oadp.png}
  \caption{All the 462284 European samples in the UK Biobank data.}
  \label{fig:ukb_eur}
\end{figure}


\end{document}

% \begin{figure}[p]
%   \centering
%   \includegraphics[width=0.98\textwidth]{n1000}
%   \caption{
%     Predicted PC scores on top of reference PC scores.
%     The four populations in the $2 \times 2$ grid simulated by the GGS software can be clearly identified.
%     Reference individuals are in color,
%     while predicted PC scores for the study individuals are in gray.
%     Two measurements of deviations (accuracy) are used.
%     The reference center error is the square root of the average squared distance between the study individuals and the population centers, which are the means of the PC scores of the reference individuals by the populations.
%     The golden standard error is the square root of the averate squared distance between the study individuals' PC scores predicted by different methods compared to the golden standard result, which in our case is the PC scores predicted by ADP.
%     The reference size is 1000 and the study size is 200.
%     There are 100,000 loci (1000 per genealogy) and the migration rate is 100.
%   }
%   \label{fig:n1000}
% \end{figure}

% \begin{figure}[p]
%   \centering
%   \includegraphics[width=0.98\textwidth]{ukb}
%   \caption{
%     The reference individuals are from the 1000 Genomes dataset with 2492 unrelated individuals from 5 superpopulations: African, American, East Asian, European, and South Asian. 
%     The study sample contain 1000 individuals from the UKBioBank dataset.
%     There are about 125,000 loci shared by the reference and study data.
%     The golden standard error is the square root of the averate squared distance between the study individuals' PC scores predicted by different methods compared to the golden standard result, which in our case is the PC scores predicted by ADP.
%   }
%   \label{fig:ukb}
% \end{figure}

% \begin{figure}[p]
%   \centering
%   \includegraphics[width=0.98\textwidth]{img/nChg}
%   \caption{
%     Runtimes and deviations for different methods as the reference sample size increases.
%     The study size is fixed to 200 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{fig:nChg}
% \end{figure}

% \begin{figure}[p]
%   \centering
%   \includegraphics[width=0.98\textwidth]{img/mChg}
%   \caption{
%     Runtimes and deviations for different methods as the study sample size increases.
%     The reference sample size is fixed to 600 and the number of SNPs is 100,000 (1000 per genealogy).
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{fig:mChg}
% \end{figure}

% \includegraphics[width=0.98\textwidth]{runtimes_rand}

% \includegraphics[width=0.98\textwidth]{err_refcenter_rand}

% \includegraphics[width=0.98\textwidth]{err_trace_rand}

% \includegraphics[width=0.98\textwidth]{err_refcenter_rand}

% \includegraphics[width=0.98\textwidth]{kgn_allChr_ukb_orphans_ukb_1k_comb}

% \includegraphics[width=0.98\textwidth]{kgn_allChr_ukb_orphans_ukb_1k_rand}

% \begin{table} 
%   \centering
%   \begin{tabular}{|l|l|l|l|l|}
%     \hline
%     Method & Reference Runtimes (n=2492) & Study Runtimes (m=1000)& Golden Standard Error \\ 
%     \hline
%     ADP & 411 & 29813 & 0.00 \\
%     \hline
%     OADP & 2518 & 103 & 0.40 \\
%     \hline
%     SP & 411 & 425 & 1.95 \\
%     \hline
%     AP & 411 & 482 & 1.12 \\
%     \hline
%   \end{tabular}
%   \caption{
%     Comparison of runtimes and accuracy for different methods applied to the real-life datasets.
%     The reference individuals are from the 1000 Genomes dataset with 2492 unrelated individuals.
%     The study sample contain 1000 individuals from the UKBioBank dataset.
%     There are about 125,000 loci shared by the reference and study data.
%     The golden standard error is the square root of the averate squared distance between the study individuals' PC scores predicted by different methods compared to the golden standard result, which in our case is the PC scores predicted by ADP.
%   }
%   \label{tbl:ukb}
% \end{table} 

% \begin{table}
%   \centering
%   \begin{tabular}{|l|l|l|l|l|l|}
%     \hline
%     Reference Size & 1000 & 1500 & 2000 & 2500 & 3000 \\
%     \hline
%     ADP        &  68  & 1263 & 2871 & 5720 & 9737 \\
%     OADP       &  72  & 18   & 19   & 22   & 24   \\
%     SP         &  68  & 26   & 42   & 67   & 96   \\
%     AP         &  68  & 69   & 107  & 91   & 119  \\
%     \hline
%   \end{tabular}
%   \caption{
%     Reference runtimes for the simulated data as the reference size increases.
%     The study size is fixed to 200 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{tbl:nChg-runtimes-ref}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{|l|l|l|l|l|l|}
%     \hline
%     Reference Size & 1000 & 1500 & 2000 & 2500 & 3000 \\
%     \hline
%     ADP        & 391  &  130 &  222 &  339 &  486 \\
%     OADP       & 15   &  136 &  230 &  350 &  499 \\
%     SP         & 16   &  130 &  222 &  339 &  486 \\
%     AP         & 34   &  130 &  222 &  339 &  486 \\
%     \hline
%   \end{tabular}
%   \caption{
%     Study runtimes for simulated data as the reference size increases.
%     The study size is fixed to 200 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{tbl:nChg-runtimes-study}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{|l|l|l|l|l|l|}
%     \hline
%     Study Size & 1000  & 1500 & 2000 & 2500 & 3000 \\
%     \hline
%     ADP        & 0.00  & 0.00 & 0.00 & 0.00 & 0.00 \\
%     OADP       & 0.23  & 0.15 & 0.11 & 0.08 & 0.07 \\
%     SP         & 6.00  & 4.34 & 3.43 & 2.75 & 2.34 \\
%     AP         & 0.46  & 0.37 & 0.31 & 0.26 & 0.22 \\
%     \hline
%   \end{tabular}
%   \caption{
%     Golden standard error for the simulated data as the reference size increases.
%     The study size is fixed to 200 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{tbl:nChg-accuracy-gold}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{|l|l|l|l|l|l|}
%     \hline
%     Reference Size & 1000  & 1500 & 2000 & 2500 & 3000 \\
%     \hline
%     ADP        & 4.53  & 4.13 & 3.81 & 3.59 & 3.23 \\
%     OADP       & 4.43  & 4.06 & 3.77 & 3.55 & 3.20 \\
%     SP         & 5.79  & 4.51 & 3.89 & 3.56 & 3.19 \\
%     AP         & 4.59  & 4.10 & 3.76 & 3.57 & 3.24 \\
%     \hline
%   \end{tabular}
%   \caption{
%     Geographic center error for the simulated data as the reference size increases.
%     The study size is fixed to 200 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{tbl:nChg-accuracy-ctr}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{|l|l|l|l|l|l|}
%     \hline
%     Reference Size & 1000 & 1500 & 2000 & 2500 & 3000 \\
%     \hline
%     ADP        & 507  & 758 & 1011 & 1262 & 1549 \\
%     OADP       &  73  & 108 &  156 &  189 &  282 \\
%     SP         &  44  &  64 &   84 &  103 &  131 \\
%     AP         &  66  &  81 &  100 &  117 &  146 \\
%     \hline
%   \end{tabular}
%   \caption{
%     Study runtimes for the simulated data as the study size increases.
%     The reference size is fixed to 600 and the number of SNPs is 100,000 (1000 per genealogy). 
%     Simulation is done on a $2 \times 2$ grid with a migration rate of 100 by the GGS software. 
%   }
%   \label{tbl:mChg-runtimes-study}
% \end{table}


% \section{Randomized SVD}\label{sec:randsvd}

% In addition to estimating the PC scores of the study individuals,
% we have another algorithm that aims at improving the decomposition of the
% training data.
% For SP, AP, ADP, and OADP, we need to find the covariance matrix of the training
% data, which has dimension $n \times n$, and conduct eigendecomosition on it.
% However, when $n$ is exceedingly large,
% computation and memory cost of PCA on the reference samples can be enormous.
% A randomized SVD (RSVD) algorithm has been developed to handle this issue.
% When the reference sample size is small, RSVD can be slower than standard SVD,
% but it is scalable to large sample size.
% For the details of the RSVD algorithm, see \citet{halko2011finding}.

% Notice that the randomized SVD method on the training data
% only calculates the first $k$ eigenvalues.
% However, in order to use adjusted projection,
% all the eigenvalues of the training data are required.
% Thus for the missing eigenvalues,
% we predict them by regressing their log-scale values on the ranks
% and using the first $k$ eigenvalues for training the linear regression.
